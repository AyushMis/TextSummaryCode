{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from Utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pdfminer.converter import TextConverter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pdfminer.layout import LAParams\n",
    "#from pdfminer.pdfpage import PDFPage\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "import nbimporter\n",
    "import Utilities as utility\n",
    "import collections\n",
    "import imp\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(sentences, vocab, dic):\n",
    "    final_score = []\n",
    "    text1 = sentences.split('.')\n",
    "    text2 = [word_tokenize(element) for element in text1]\n",
    "    #print(text2)\n",
    "    for i in range(len(text2)):\n",
    "        score=0\n",
    "        for j in range(len(text2[i])):\n",
    "            if text2[i][j] in vocab:\n",
    "                score+=dic[text2[i][j]]\n",
    "        temp = [score,text1[i]]\n",
    "        final_score.append(temp)\n",
    "    final_score.sort(reverse=True)\n",
    "    return(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the compression ratio in percentege: 40\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(utility.loadObj('sentencesTXT')).lower()\n",
    "#print(text[:7095])\n",
    "# text = \"\"\"Criteria of compatibility of a system of linear Diophantine equations, strict inequations,\n",
    "# and nonstrict inequations are considered. Upper bounds for components of a minimal set\n",
    "# of solutions and algorithms of construction of minimal generating sets of solutions for all\n",
    "# types of systems are given. These criteria and the corresponding algorithms for\n",
    "# constructing a minimal supporting set of solutions can be used in solving all the\n",
    "# considered types of systems and systems of mixed types.\"\"\"\n",
    "\n",
    "compression_ratio = int(input(\"Enter the compression ratio in percentege: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text, vocab = utility.removeStopWords(text)\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurenceMatrix = utility.findOccurenceMatrix(clean_text, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "degreeWord = occurenceMatrix.sum(axis=1)\n",
    "freqWord = np.amax(occurenceMatrix, axis=1)\n",
    "score = degreeWord/freqWord\n",
    "dic = {}\n",
    "for i in range(len(vocab)):\n",
    "    dic.update({vocab[i]: score[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_scores = []\n",
    "for element in clean_text:\n",
    "    scr = 0\n",
    "    for word in element:\n",
    "        scr += dic[word]\n",
    "    candidate_scores.append((scr, ' '.join(element)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_scores.sort(reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #faltu work here\n",
    "# tokens = word_tokenize(text)\n",
    "# #print(tokens)\n",
    "# full = []\n",
    "# for i in range(len(clean_text)-1):\n",
    "#     temp = []\n",
    "#     index1 = tokens.index(clean_text[i][-1])\n",
    "#     index2 = tokens.index(clean_text[i+1][0])\n",
    "#     temp.append(' '.join(clean_text[i]))\n",
    "#     if index2-index1==2:\n",
    "#         temp.append(tokens[index1+1])\n",
    "#         temp.append(' '.join(clean_text[i+1]))\n",
    "#     full.append(temp)\n",
    "# #full = [' '.join(element) for element in full]\n",
    "# for i in range(len(full)-1):\n",
    "#     if full[i][-1]==full[i+1][0]:\n",
    "#         temp = full[i+1][1:]\n",
    "#         #print(temp)\n",
    "#         full[i] = full[i]+temp\n",
    "#         print(full[i])\n",
    "#         del(full[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_sentences = ranking(text, vocab, dic)\n",
    "#print(score_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_summary = int((compression_ratio/100)*len(text.split('.')))\n",
    "summary_RAKE = []\n",
    "for i in range(length_summary):\n",
    "    summary_RAKE.append(scored_sentences[i][1])\n",
    "#print(summary_RAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.saveObj(summary_RAKE,'RakeSummary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
